{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/Documents/UiS/Thesis/dataset/allsides-collection/data_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "From the Right     2542\nFrom the Left      2388\nFrom the Center    1517\nName: bias, dtype: int64"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "data['bias'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_data = data[data['bias']=='From the Left']\n",
    "right_data = data[data['bias']=='From the Right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Avg. len valid1:  9.248743718592964\nAvg. len train1:  23.992421134454688\nAvg. len valid2:  9.961447678992918\nAvg. len train2:  21.97825974093399\n"
    }
   ],
   "source": [
    "train1 = []\n",
    "train2 = []\n",
    "valid1 = []\n",
    "valid2 = []\n",
    "\n",
    "length = 0\n",
    "for s in left_data['title']:\n",
    "    tokens = word_tokenize(s)\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    length += len(words)\n",
    "    valid1.append(\" \".join(words))\n",
    "\n",
    "print(\"Avg. len valid1: \", length/len(valid1))\n",
    "\n",
    "train1.extend(valid1)\n",
    "for content in left_data['original_body']:\n",
    "    try:\n",
    "        sentences = nltk.sent_tokenize(content)\n",
    "        for s in sentences:\n",
    "            tokens = word_tokenize(s)\n",
    "            words = [word for word in tokens if word.isalpha()]\n",
    "            length += len(words)\n",
    "            train1.append(\" \".join(words))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Avg. len train1: \", length/len(train1))\n",
    "\n",
    "length = 0\n",
    "for s in right_data['title']:\n",
    "    tokens = word_tokenize(s)\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    length += len(words)\n",
    "    valid2.append(\" \".join(words))\n",
    "\n",
    "print(\"Avg. len valid2: \", length/len(valid2))\n",
    "\n",
    "train2.extend(valid2)\n",
    "for content in right_data['original_body']:\n",
    "    try:\n",
    "        sentences = nltk.sent_tokenize(content)\n",
    "        for s in sentences:\n",
    "            tokens = word_tokenize(s)\n",
    "            words = [word for word in tokens if word.isalpha()]\n",
    "            length += len(words)\n",
    "            train2.append(\" \".join(words))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Avg. len train2: \", length/len(train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(filepath, text):\n",
    "    with open(filepath,'w') as file:\n",
    "        for line in text:\n",
    "            if isinstance(line, str):\n",
    "                file.write(line)\n",
    "                file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file('webis/train1.txt', train1)\n",
    "write_file('webis/train2.txt', train2)\n",
    "write_file('webis/valid1.txt', valid1)\n",
    "write_file('webis/valid2.txt', valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}