{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_and_content(\n",
    "    data_file, ground_truth_file, \n",
    "    title_output_file1, title_output_file2,\n",
    "    content_output_file1='', content_output_file2=''):\n",
    "    \n",
    "    gt_tree = ET.parse(ground_truth_file)\n",
    "    gt_root = gt_tree.getroot()\n",
    "    \n",
    "    ground_truth = {}\n",
    "    for child in gt_root:\n",
    "        ground_truth[child.attrib['id']] = child.attrib['bias']\n",
    "    \n",
    "    tree = ET.parse(data_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    title_f1 = open(title_output_file1, 'w+')\n",
    "    title_f2 = open(title_output_file2, 'w+')\n",
    "    if content_output_file1 and content_output_file2:\n",
    "        content_f1 = open(content_output_file1, 'w+')\n",
    "        content_f2 = open(content_output_file2, 'w+')\n",
    "    for article in root:\n",
    "        # process titles\n",
    "        words = [word for word in nltk.word_tokenize(article.attrib['title'].lower()) if re.sub(r'[^\\w\\s]', '', word) != '']\n",
    "        if len(words):\n",
    "            words.append('.\\n')\n",
    "            title = \" \".join(words)\n",
    "            if article.attrib['id'] in ground_truth:\n",
    "                if ground_truth[article.attrib['id']] == 'left':\n",
    "                    title_f1.write(title)\n",
    "                elif ground_truth[article.attrib['id']] == 'right':\n",
    "                    title_f2.write(title)\n",
    "                \n",
    "        # process content\n",
    "        if content_output_file1 and content_output_file2:\n",
    "            content = []\n",
    "            for p in article.findall('p'):\n",
    "                if p.text:\n",
    "                    words = [word for word in nltk.word_tokenize(p.text.lower()) if re.sub(r'[^\\w\\s]', '', word) != '']\n",
    "                    if len(words):\n",
    "                        words.append('.')\n",
    "                        sentence = \" \".join(words)\n",
    "                        if article.attrib['id'] in ground_truth:\n",
    "                            if ground_truth[article.attrib['id']] == 'left':\n",
    "                                content_f1.write(sentence + \"\\n\")\n",
    "                            elif ground_truth[article.attrib['id']] == 'right':\n",
    "                                content_f2.write(sentence + \"\\n\")\n",
    "                # content.append(sentence)\n",
    "        # content = \" \".join(content) + \"\\n\"\n",
    "        # if article.attrib['id'] in ground_truth and ground_truth[article.attrib['id']] == 'true':\n",
    "        #     content_f1.write(content)\n",
    "        # else:\n",
    "        #     content_f2.write(content)\n",
    "\n",
    "    title_f1.close()\n",
    "    title_f2.close()\n",
    "    if content_output_file1 and content_output_file2:\n",
    "        content_f1.close()\n",
    "        content_f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'bias'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a0bcb7eddd0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m'pan-data2/title/train2.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5c7c03bcc894>\u001b[0m in \u001b[0;36mget_title_and_content\u001b[0;34m(data_file, ground_truth_file, title_output_file1, title_output_file2, content_output_file1, content_output_file2)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0mtitle_f1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'right'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bias'"
     ]
    }
   ],
   "source": [
    "# get_title_and_content(\n",
    "#     '../dataset/articles-training-bypublisher-20181122.xml',\n",
    "#     '../dataset/ground-truth-training-bypublisher-20181122.xml',\n",
    "#     'pan-data/title/train1.txt',\n",
    "#     'pan-data/title/train2.txt',\n",
    "#     'pan-data/content/train1.txt',\n",
    "#     'pan-data/content/train2.txt'\n",
    "# )\n",
    "\n",
    "# get_title_and_content(\n",
    "#     '../dataset/articles-validation-bypublisher-20181122.xml',\n",
    "#     '../dataset/ground-truth-validation-bypublisher-20181122.xml',\n",
    "#     'pan-data/title/valid1.txt',\n",
    "#     'pan-data/title/valid2.txt',\n",
    "#     'pan-data/content/valid1.txt',\n",
    "#     'pan-data/content/valid2.txt'\n",
    "# )\n",
    "\n",
    "get_title_and_content(\n",
    "    '../dataset/articles-training-bypublisher-20181122.xml',\n",
    "    '../dataset/ground-truth-training-bypublisher-20181122.xml',\n",
    "    'pan-data2/title/train1.txt',\n",
    "    'pan-data2/title/train2.txt',\n",
    "    '',\n",
    "    ''\n",
    ")\n",
    "\n",
    "get_title_and_content(\n",
    "    '../dataset/articles-validation-bypublisher-20181122.xml',\n",
    "    '../dataset/ground-truth-validation-bypublisher-20181122.xml',\n",
    "    'pan-data2/title/valid1.txt',\n",
    "    'pan-data2/title/valid2.txt',\n",
    "    '',\n",
    "    ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_title_and_content(\n",
    "    '../dataset/articles-training-byarticle-20181122.xml',\n",
    "    '../dataset/ground-truth-training-byarticle-20181122.xml',\n",
    "    'pan-data-article/title/train1.txt',\n",
    "    'pan-data-article/title/train2.txt',\n",
    "    'pan-data-article/content/train1.txt',\n",
    "    'pan-data-article/content/train2.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}