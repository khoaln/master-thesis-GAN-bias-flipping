{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_and_content(\n",
    "    data_file, ground_truth_file, \n",
    "    title_output_file1, title_output_file2,\n",
    "    content_output_file1='', content_output_file2=''):\n",
    "    \n",
    "    gt_tree = ET.parse(ground_truth_file)\n",
    "    gt_root = gt_tree.getroot()\n",
    "    \n",
    "    ground_truth = {}\n",
    "    for child in gt_root:\n",
    "        ground_truth[child.attrib['id']] = child.attrib['bias']\n",
    "    \n",
    "    tree = ET.parse(data_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    title_f1 = open(title_output_file1, 'w+')\n",
    "    title_f2 = open(title_output_file2, 'w+')\n",
    "    if content_output_file1 and content_output_file2:\n",
    "        content_f1 = open(content_output_file1, 'w+')\n",
    "        content_f2 = open(content_output_file2, 'w+')\n",
    "    for article in root:\n",
    "        # process titles\n",
    "        words = [word for word in nltk.word_tokenize(article.attrib['title'].lower()) if re.sub(r'[^\\w\\s]', '', word) != '']\n",
    "        if len(words):\n",
    "            words.append('.\\n')\n",
    "            title = \" \".join(words)\n",
    "            if article.attrib['id'] in ground_truth:\n",
    "                if ground_truth[article.attrib['id']] == 'left':\n",
    "                    title_f1.write(title)\n",
    "                elif ground_truth[article.attrib['id']] == 'right':\n",
    "                    title_f2.write(title)\n",
    "                \n",
    "        # process content\n",
    "        if content_output_file1 and content_output_file2:\n",
    "            content = []\n",
    "            for p in article.findall('p'):\n",
    "                if p.text:\n",
    "                    words = [word for word in nltk.word_tokenize(p.text.lower()) if re.sub(r'[^\\w\\s]', '', word) != '']\n",
    "                    if len(words):\n",
    "                        words.append('.')\n",
    "                        sentence = \" \".join(words)\n",
    "                        if article.attrib['id'] in ground_truth:\n",
    "                            if ground_truth[article.attrib['id']] == 'left':\n",
    "                                content_f1.write(sentence + \"\\n\")\n",
    "                            elif ground_truth[article.attrib['id']] == 'right':\n",
    "                                content_f2.write(sentence + \"\\n\")\n",
    "                # content.append(sentence)\n",
    "        # content = \" \".join(content) + \"\\n\"\n",
    "        # if article.attrib['id'] in ground_truth and ground_truth[article.attrib['id']] == 'true':\n",
    "        #     content_f1.write(content)\n",
    "        # else:\n",
    "        #     content_f2.write(content)\n",
    "\n",
    "    title_f1.close()\n",
    "    title_f2.close()\n",
    "    if content_output_file1 and content_output_file2:\n",
    "        content_f1.close()\n",
    "        content_f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_title_and_content(\n",
    "#     '../dataset/articles-training-bypublisher-20181122.xml',\n",
    "#     '../dataset/ground-truth-training-bypublisher-20181122.xml',\n",
    "#     'pan-data/title/train1.txt',\n",
    "#     'pan-data/title/train2.txt',\n",
    "#     'pan-data/content/train1.txt',\n",
    "#     'pan-data/content/train2.txt'\n",
    "# )\n",
    "\n",
    "# get_title_and_content(\n",
    "#     '../dataset/articles-validation-bypublisher-20181122.xml',\n",
    "#     '../dataset/ground-truth-validation-bypublisher-20181122.xml',\n",
    "#     'pan-data/title/valid1.txt',\n",
    "#     'pan-data/title/valid2.txt',\n",
    "#     'pan-data/content/valid1.txt',\n",
    "#     'pan-data/content/valid2.txt'\n",
    "# )\n",
    "\n",
    "get_title_and_content(\n",
    "    '../dataset/articles-training-bypublisher-20181122.xml',\n",
    "    '../dataset/ground-truth-training-bypublisher-20181122.xml',\n",
    "    'pan-data2/title/train1.txt',\n",
    "    'pan-data2/title/train2.txt',\n",
    "    '',\n",
    "    ''\n",
    ")\n",
    "\n",
    "get_title_and_content(\n",
    "    '../dataset/articles-validation-bypublisher-20181122.xml',\n",
    "    '../dataset/ground-truth-validation-bypublisher-20181122.xml',\n",
    "    'pan-data2/title/valid1.txt',\n",
    "    'pan-data2/title/valid2.txt',\n",
    "    '',\n",
    "    ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_title_and_content(\n",
    "    '../dataset/articles-training-byarticle-20181122.xml',\n",
    "    '../dataset/ground-truth-training-byarticle-20181122.xml',\n",
    "    'pan-data-article/title/train1.txt',\n",
    "    'pan-data-article/title/train2.txt',\n",
    "    'pan-data-article/content/train1.txt',\n",
    "    'pan-data-article/content/train2.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}